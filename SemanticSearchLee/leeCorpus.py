'''
Create LSI Model on Lee Corpus
'''

import logging
from pprint import pprint

from gensim import corpora
from gensim import models
from gensim import similarities
from gensim.test.utils import get_tmpfile
from nltk.corpus import stopwords


def generateSimilarityIndex(corpus, num_topics=100):
    ##############################################################
    # Create TFIDF and LSI Models on the corpus
    ##############################################################
    tfidfModel = models.TfidfModel(corpus)
    corpus_tfidf = tfidfModel[corpus]

    # Reduce to 100 dimensions
    lsiModel = models.LsiModel(corpus_tfidf, id2word=dictionary,
                               num_topics=num_topics)  # initialize an LSI transformation
    # lsi_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=10)  # initialize an LSI transformation
    # lsi_model = models.HdpModel(corpus_tfidf, id2word=dictionary)  # initialize an LSI transformation
    corpus_lsi = lsiModel[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi

    # Print the topics generated by the lsi model
    lsiModel.print_topics()

    # Create an index on which if we hit we will get the
    # similarity of the hitting object with all these documents
    # in LSI space

    # In memory computation - better for small datasets that fit in memory
    # index = similarities.MatrixSimilarity(corpus_lsi)  # transform corpus to LSI space and index it

    # Non In memory computation - better for big datasets that dont fit in memory
    index_temp = get_tmpfile("lsimodel")  # create a temporary file named lsimodel to save things
    # Note that you need to give num_topics again here as num_features
    index = similarities.Similarity(index_temp, corpus_lsi,
                                    num_features=num_topics)  # transform corpus to LSI space and index it

    return [index, tfidfModel, lsiModel]


if __name__ == '__main__':
    print 'Creates LSI Model on Lee Corpus'

    # Import the logging module for gensim
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    # Load the dataset
    f = open('./rawData.csv', 'rb')
    data = f.readlines()

    documents = []
    for document in data:
        documents.append(document)

    # pprint(documents)

    # Remove common words and tokenize
    stoplist = set('for a of the and to in'.split())

    texts = []
    i = 0
    idToDocumentDict = {}  # id-> document
    for document in documents:
        s = [word for word in document.lower().split() if word not in stoplist]
        s = [x for x in s if x not in set(stopwords.words('english'))]
        texts.append(s)

        idToDocumentDict[i] = document
        i += 1

    # Create a dictionary
    dictionary = corpora.Dictionary(texts)
    # pprint(dictionary)

    # Create a corpus
    corpus = [dictionary.doc2bow(text) for text in texts]
    # pprint(corpus)

    # Create the TFIDF and LSI Models
    [index, tfidfModel, lsiModel] = generateSimilarityIndex(corpus, num_topics=100)

    # Transform the search keyword to lsi space
    search_text = 'Afghan security'.lower().split()

    # Get its bag of words
    corpus_search_keyword = dictionary.doc2bow(search_text)
    # print 'corpus_search_keyword:', corpus_search_keyword

    tfidf_search_keyword = tfidfModel[corpus_search_keyword]
    # print 'tfidf_search_keyword:', tfidf_search_keyword

    lsi_search_keyword = lsiModel[tfidf_search_keyword]
    # print 'lsi_search_keyword:',lsi_search_keyword

    #################################################################
    # find the similarity of search keyword to the documents in lsi space

    sims = index[lsi_search_keyword]  # perform a similarity query against the corpus
    # print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples

    sims = sorted(enumerate(sims), key=lambda item: -item[1])

    # sims = sorted(enumerate(sims), key=lambda item: -item[-1][1])

    count = 0
    articles2bsent = []
    maxArticlesTobSent = 10
    for internal_id, (document_id, lsicosine) in enumerate(sims):
        article = idToDocumentDict[document_id]
        # print(internal_id, (document_id, lsicosine), article)

        articles2bsent.append(article)
        if count >= maxArticlesTobSent:
            break

        count += 1

    pprint(articles2bsent)
